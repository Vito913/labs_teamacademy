{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Web Scraping with Selenium\n",
    "\n",
    "## Overview\n",
    "In this lab, we will use Selenium to automate web scraping tasks. We will scrape product data from Amazon and AliExpress, preprocess and clean the data, perform exploratory data analysis, and develop a simple prediction algorithm.\n",
    "\n",
    "## Key Topics\n",
    "- **Selenium Overview**: Introduction to Selenium for browser automation.\n",
    "- **Scraper Class**: Implementation of a `Scraper` class for web scraping.\n",
    "- **Amazon Scraping**: Extract product details, descriptions, and reviews from Amazon.\n",
    "- **AliExpress Scraping**: Extract product details from AliExpress.\n",
    "- **Data Preprocessing**: Clean and preprocess the scraped data.\n",
    "- **Data Merging**: Merge data from multiple sources.\n",
    "- **Exploratory Data Analysis**: Analyze data distributions and relationships.\n",
    "- **Prediction Algorithm**: Develop and apply a prediction algorithm for product sales.\n",
    "- **Data Visualization**: Visualize the results of the prediction algorithm.\n",
    "- **Interactive Data Exploration**: Use interactive elements to explore the data.\n",
    "\n",
    "## Objectives\n",
    "- Understand the basics of Selenium for web scraping.\n",
    "- Implement a web scraper using the `Scraper` class.\n",
    "- Scrape and preprocess data from Amazon and AliExpress.\n",
    "- Perform exploratory data analysis and visualize the results.\n",
    "- Develop a simple prediction algorithm and apply it to the data.\n",
    "- Explore the data interactively.\n",
    "\n",
    "## Steps\n",
    "1. **Setup and Installations**: Install required packages.\n",
    "2. **Import Libraries**: Import necessary libraries for web scraping and data analysis.\n",
    "3. **Scraper Class Implementation**: Define and use the `Scraper` class.\n",
    "4. **Amazon Data Scraping**: Scrape product details and reviews from Amazon.\n",
    "5. **AliExpress Data Scraping**: Scrape product details from AliExpress.\n",
    "6. **Data Preprocessing**: Clean and preprocess the scraped data.\n",
    "7. **Data Merging**: Merge and standardize data from multiple sources.\n",
    "8. **Exploratory Data Analysis**: Perform and visualize exploratory data analysis.\n",
    "9. **Prediction Algorithm**: Develop and apply a prediction algorithm.\n",
    "10. **Interactive Exploration**: Use interactive widgets to explore the data.\n",
    "\n",
    "By the end of this lab, you will have a comprehensive understanding of web scraping with Selenium and how to preprocess, analyze, and visualize the scraped data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib numpy beautifulsoup4 selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from information_types import Product\n",
    "# Ensure inline plotting for Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Selenium\n",
    "\n",
    "Selenium is a powerful tool for controlling web browsers through programs and performing browser automation. It is primarily used for testing web applications but can also be used for web scraping and automating repetitive web-based tasks.\n",
    "\n",
    "#### Key Features of Selenium:\n",
    "- **Cross-Browser Support**: Selenium supports multiple browsers like Chrome, Firefox, Safari, and Edge.\n",
    "- **Automation**: It can automate web interactions such as clicking buttons, filling forms, and navigating through pages.\n",
    "- **Dynamic Content Handling**: Selenium can handle dynamic content that is loaded via JavaScript, which is not possible with static HTML parsers.\n",
    "- **Browser Simulation**: Selenium simulates a real browser, making it less likely to be blocked by websites that detect and prevent automated scraping.\n",
    "  \n",
    "The last two points are the real reason why we will be using Selenium in this notebook. BeautifulSoup and Requests are great for static web pages, but they cannot handle dynamic content that is loaded via JavaScript. Also sometimes when trying to scrape a website, the website might detect that we are using a bot and block our requests. Selenium can help us overcome these limitations by simulating a real user interacting with the website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Not Just Use BeautifulSoup with Requests?\n",
    "\n",
    "While BeautifulSoup and Requests are excellent tools for web scraping, they have limitations compared to Selenium:\n",
    "\n",
    "- **Static Content**: BeautifulSoup and Requests can only scrape static content that is available in the initial HTML response. They cannot handle dynamic content loaded via JavaScript.\n",
    "- **JavaScript Execution**: Selenium can execute JavaScript, allowing it to interact with elements that are dynamically generated or modified after the initial page load.\n",
    "- **Complex Interactions**: Selenium can perform complex interactions like clicking buttons, filling forms, and navigating through multiple pages, which is challenging with just BeautifulSoup and Requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper class\n",
    "The `Scraper` class is designed to facilitate web scraping using Selenium with Chrome WebDriver. \n",
    "\n",
    "#### Key Features:\n",
    "- **Headless Mode**: Allows the scraper to run without opening a browser window, which is useful for running scripts on servers or in the background.\n",
    "- **Image Loading Control**: Option to disable image loading for faster scraping.\n",
    "- **Customizable Options**: Additional Chrome options can be set for the WebDriver.\n",
    "- **Window Size**: Sets the window size for the browser.\n",
    "\n",
    "#### Initialization Parameters:\n",
    "- `headless` (bool): If `True`, runs the scraper in headless mode.\n",
    "- `load_images` (bool): If `False`, disables image loading.\n",
    "- `options` (Options): Chrome options for the WebDriver.\n",
    "- `window_size` (tuple): Sets the window size for the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_show:\n",
    "    def __init__(self, headless = True,\n",
    "        load_images = False, # for faster scraping we can turn off image loading\n",
    "        options = Options(), # options for the web surfer\n",
    "        window_size = (700,900)):\n",
    "\n",
    "        if headless:\n",
    "            # if headless is True, we can run the scraper \n",
    "            # without opening a browser\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            \n",
    "        prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "        if not load_images:\n",
    "            options.add_experimental_option(\"prefs\", prefs)\n",
    "        # adding some more options to the web surfer\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        # creating the websurfer using chrome\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.driver.set_window_size(*window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to use the `Scraper` class to scrape a website:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from information_types import Scraper\n",
    "scraper = Scraper(headless=False,load_images=True) # creating the scraper object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.driver.get(\"https://www.english-hatter.nl\") # opening the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### closing the browser\n",
    "\n",
    "with the command below we close the browser. we will still need this instance to scrape the data from the website.\n",
    "So after we close it we should reinitialize it again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.driver.quit() # closing the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Scraper(headless=False,load_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usecase Amazon\n",
    "\n",
    "We want to get all the data from amazons webpage for a given search term. We will use the search term `\"laptop\"` and scrape the first page of the search results. The code below will scrape the product name, price, and rating for each product on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# go to the url containing the search results\n",
    "scraper.driver.get(\"https://www.amazon.nl/s?k=laptop&language=en_GB\")\n",
    "# used to store the products\n",
    "products = []\n",
    "\n",
    "# get the html data in a format that we can use easily\n",
    "html_data = BeautifulSoup(scraper.driver.page_source, 'html.parser')\n",
    "\n",
    "# Based on the structure of the html data, we can find the products\n",
    "products = html_data.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "# loop through the products and get the required information\n",
    "for product in products:\n",
    "    # we know that the title is in a span tag with the class a-text-normal\n",
    "    title = product.find('span', {'class': 'a-text-normal'}).text\n",
    "    print(title)\n",
    "    # used to get the all the elements that are related to the price\n",
    "    price_symbol = product.find('span', {'class': 'a-price-symbol'})\n",
    "    if price_symbol is not None:\n",
    "        price_symbol = price_symbol.text\n",
    "    price = product.find('span', {'class': 'a-price-whole'})\n",
    "    price_decimal = product.find('span', {'class': 'a-price-fraction'})\n",
    "    if price and price_decimal:\n",
    "        full_price = price.text.replace(',', '') + price_decimal.text\n",
    "        print(full_price + price_symbol)\n",
    "    else:\n",
    "        print('Price not available')\n",
    "    url = product.find(\"a\",{\"class\":\"a-link-normal s-no-outline\"})['href']\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part where we get the link is the most important as that will allow us to scrape the data from the product page. We will use the link to go to the product page and scrape additional information like the product description, specifications, and reviews.\n",
    "So now we can focus on getting the desired information from the product page.\n",
    "\n",
    "### Usecase Amazon Product Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.driver.get(\"https://www.amazon.nl/Aqara-Aanwezigheids-zonepositionering-multi-persoon-val-detectie/dp/B0BXWZMQJ3/ref=sr_1_1_sspa?__mk_nl_NL=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=21JVASD4BUSV7&dib=eyJ2IjoiMSJ9.NPyacmd0m85ZeCNArVPM9Bon6Y8TLrSQcO9BdCAO0vDsR2arYYQrWnOVgQK5_U968FCJnBVFq6IaJqXyvgpQNZgCCpdhZVI68wwMkuBDdTN7BE4mdQelc53TzCAMMio3UkQKa-x_IZ6i39k3zpA_IGomSlEcAiDewcidR7y8WKv-vdhlCqK_mT3z7aob5B8-pwYrwiAreNvD-WpE_TWeRcBRUT2bgDrGfoWt2PIe-ADqIag3e2do1CRWBanpEE9wAGnVNMfaVC1dzKbcaHCnlWaoJ3ccRbgMqwqsylaB3uc.KnvdkEs2tzfp7_Z-JoAal-rD1uenvIzuky1B7BtI12I&dib_tag=se&keywords=sensors&qid=1732036910&sprefix=sensors%2Caps%2C97&sr=8-1-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1&language=en_GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# once again we get the html data in a format that we can use easily\n",
    "page_data = BeautifulSoup(scraper.driver.page_source, 'html.parser')\n",
    "\n",
    "# get the elements containing the price information\n",
    "price_element = page_data.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "price_decimal_element = page_data.find(\"span\", {\"class\": \"a-price-fraction\"})\n",
    "price_symbol_element = page_data.find(\"span\", {\"class\": \"a-price-symbol\"})\n",
    "\n",
    "# if the elements are found, we can get the price information\n",
    "if price_element and price_decimal_element and price_symbol_element:\n",
    "\tprice = price_element.text.replace(',', '')\n",
    "\tprice_decimal = price_decimal_element.text\n",
    "\tfull_price = price + price_decimal\n",
    "\tprice_symbol = price_symbol_element.text\n",
    "\tprint(full_price + price_symbol)\n",
    "else:\n",
    "\tprint(\"Price information not found\")\n",
    "\n",
    "# get the product name\n",
    "product_name_element = page_data.find(\"span\", {\"id\": \"productTitle\"})\n",
    "print(product_name_element.text.strip())\n",
    "\n",
    "# get the product description\n",
    "labels_element = page_data.find(\"ul\", {\"class\": \"a-unordered-list a-vertical a-spacing-mini\"})\n",
    "labels = []\n",
    "\n",
    "# if the labels are found, we can get the product description\n",
    "if labels_element:\n",
    "\tlist_items = labels_element.find_all(\"li\", {\"class\": \"a-spacing-mini\"})\n",
    "\tfor item in list_items:\n",
    "\t\tlabel = item.find(\"span\", {\"class\": \"a-list-item\"}).text.strip()\n",
    "\t\tlabels.append(label)\n",
    "\tprint(labels)\n",
    "else:\n",
    "\tprint(\"Labels not found\")\n",
    "\n",
    "# wait for the rating element to be present\n",
    "\n",
    "rating_element = page_data.select_one(\"#acrPopover\")\n",
    "\t\n",
    "if rating_element:\n",
    "\ttext_rating = rating_element.get(\"title\")\n",
    "\tif isinstance(text_rating,str):\n",
    "\t\trating = float(text_rating.split(\" \")[0].replace(\",\",\".\"))\n",
    "\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon reviews\n",
    "\n",
    "We want to see what the people are saying about the product. We will scrape the reviews for a given product. The code below will scrape the review \n",
    "- title\n",
    "- rating\n",
    "- review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the page we were on we can search for the results\n",
    "reviews = page_data.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "\n",
    "# loop through the reviews and get the required information\n",
    "for review in reviews:\n",
    "    title_element = review.find(\"a\", {\"data-hook\": \"review-title\"})\n",
    "    if title_element:\n",
    "        title = title_element.text.strip().split('\\n', 1)[-1]\n",
    "    else:\n",
    "        title_element = review.find(\"span\", {\"class\": \"cr-original-review-content\"})\n",
    "        title = title_element.text.strip().split('\\n', 1)[-1] if title_element else \"Title not found\"\n",
    "    \n",
    "    review_text_element = review.find(\"span\", {\"data-hook\": \"review-body\"})\n",
    "    if review_text_element:\n",
    "        review_text = review_text_element.text.strip().replace('Read more', '').strip()\n",
    "    else:\n",
    "        review_text_element = review.find(\"span\", {\"class\": \"cr-original-review-content\"})\n",
    "        review_text = review_text_element.text.strip().replace('Read more', '').strip() if review_text_element else \"Review text not found\"\n",
    "    \n",
    "    rating_element = review.find(\"i\", {\"data-hook\": \"review-star-rating\"})\n",
    "    if not rating_element:\n",
    "        rating_element = review.find(\"span\", {\"class\": \"a-icon-alt\"})\n",
    "    if rating_element:\n",
    "        rating_text = rating_element.text.strip()\n",
    "        rating_value = float(rating_text.split()[0].replace(',', '.'))\n",
    "    else:\n",
    "        rating_value = 0.0\n",
    "    \n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Rating: {rating_value}\")\n",
    "    print(f\"Review: {review_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AliExpress Scraping\n",
    "\n",
    "I have create functions to scrape the product details from AliExpress. The functions and they work similar to the Amazon functions. The only difference is the class names and the structure of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_product_data_AEx import get_product_page_data_AE, get_product_urls_AE\n",
    "\n",
    "# Loop over the products and get the required information\n",
    "\n",
    "product_to_search = \"Butt plug\"\n",
    "scraper = Scraper(headless=False, load_images=True)\n",
    "\n",
    "link_list: list[str] = get_product_urls_AE(scraper=scraper, search_query=product_to_search, max_page_number=1)\n",
    "\n",
    "# Initialize an empty list to store products\n",
    "products:list[Product] = []\n",
    "\n",
    "# Loop over the products and get the required information\n",
    "for url in link_list:\n",
    "    product = get_product_page_data_AE(scraper, url)\n",
    "    products.append(product)\n",
    "    print(product)\n",
    "\n",
    "Product.save_product_data(products, \"aliexpress_products.csv\")\n",
    "print(\"Data saved to aliexpress_products.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have AliExpress data we also would like to get all of the amazon data. We will use the functions we created for Amazon and scrape the data for a specific search term. The code for this has been extracted from the previous examples and we now go through the process of scraping the data for Amazon.\n",
    "\n",
    "### Scrape all Amazon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_product_data_az import get_product_data_az, get_product_urls_az\n",
    "product_to_search = \"Butt plug\"\n",
    "all_products: list[Product] = []\n",
    "product_urls: list[str] = get_product_urls_az(scraper=scraper, search_query=product_to_search, max_page_number=1)\n",
    "for url in product_urls:\n",
    "    product_data = get_product_data_az(scraper=scraper, url=url)\n",
    "    all_products.append(product_data)\n",
    "\n",
    "Product.save_product_data(all_products, \"amazon_products.csv\")\n",
    "scraper.driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and Clean Data\n",
    "\n",
    "Before we can do anything useful with the data, we need to preprocess and clean it. This involves tasks like removing duplicates, handling missing values, and converting data to the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Preprocess and Clean Data\n",
    "ae_df = pd.read_csv(\"aliexpress_products.csv\")\n",
    "ae_df[\"price\"] = ae_df[\"price\"].str.replace(\"€\", \"\").str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "# Convert the string representation of the list back to a list\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "ae_df[\"about_product\"] = ae_df[\"about_product\"].apply(lambda x: safe_literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Instead of using a list of about_product, we can use a single string\n",
    "ae_df[\"about_product\"] = ae_df[\"about_product\"].apply(lambda x: \" \".join(x))\n",
    "print(ae_df.head())\n",
    "\n",
    "# Preprocess and Clean Data for amazon data\n",
    "az_df = pd.read_csv(\"amazon_products.csv\")\n",
    "az_df[\"price\"] = az_df[\"price\"].astype(str).str.replace(\"€\", \"\").str.replace(\",\", \".\").astype(float)\n",
    "print(az_df.head())\n",
    "\n",
    "full_df = pd.concat([ae_df, az_df], ignore_index=True)\n",
    "\n",
    "# Remove emojis from reviews\n",
    "full_df[\"reviews\"] = full_df[\"reviews\"].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "\n",
    "# Remove newline characters from reviews\n",
    "full_df[\"reviews\"] = full_df[\"reviews\"].str.replace(\"\\n\", \" \")\n",
    "\n",
    "# Remove any remaining non-ASCII characters\n",
    "full_df[\"reviews\"] = full_df[\"reviews\"].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "\n",
    "# Handle missing values\n",
    "full_df.fillna('', inplace=True)\n",
    "\n",
    "# Convert reviews to string type\n",
    "full_df[\"reviews\"] = full_df[\"reviews\"].astype(str)\n",
    "\n",
    "# Ensure that the reviews are properly cleaned and formatted\n",
    "full_df[\"reviews\"] = full_df[\"reviews\"].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "print(full_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Perform exploratory analysis to understand data distributions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "full_df.describe()\n",
    "\n",
    "# Distribution of product prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(full_df['price'], bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of Product Prices')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Distribution of number of reviews\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(full_df['reviews'], bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of Number of Reviews')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of price vs number of reviews\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(full_df['price'], full_df['reviews'], alpha=0.7)\n",
    "plt.title('Price vs Number of Reviews')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
