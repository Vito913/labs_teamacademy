{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping\n",
    "\n",
    "Welcome to the web scraping workshop. In this workshop, you will learn the fundamental concepts and techniques used to extract valuable insight from websites and large Data structures. Data mining is a crucial skill in today's data-driven world, enabling you to uncover patterns, trends, and relationships within data that can inform decision-making and drive business success.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Introduction to Python Web Scraping**\n",
    "    - Overview of Python libraries\n",
    "    - Requests library\n",
    "    - BeautifulSoup library\n",
    "    - Pandas library\n",
    "2. **Making HTTP Requests with Requests**\n",
    "    - Basic GET request\n",
    "    - Handling different HTTP methods (POST, PUT, DELETE)\n",
    "    - Using headers with requests\n",
    "3. **Parsing HTML with BeautifulSoup**\n",
    "    - Creating a BeautifulSoup object\n",
    "    - Navigating and searching the parse tree\n",
    "    - Extracting data from HTML tags\n",
    "4. **Data Extraction and Storage**\n",
    "    - Extracting specific elements\n",
    "    - Formatting data for storage\n",
    "    - Saving data with Pandas\n",
    "5. **Data Manipulation and Cleaning with Pandas**\n",
    "    - Creating and saving DataFrames\n",
    "    - Reading data from files\n",
    "    - Data manipulation operations\n",
    "    - Data cleaning operations\n",
    "6.  **Exercises**\n",
    "    - Basic GET request\n",
    "    - Finding specific HTML elements\n",
    "    - Creating and saving DataFrames\n",
    "    - Data statistics and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to install all the libraries that we will be needing for this workshop. To run a cell press a button that appears when you hover over the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction to Python Web Scraping\n",
    "\n",
    "Here is an overview of all the python libraries that we will be using in this workshop:\n",
    "\n",
    "- **Requests**: A simple HTTP library that allows you to send HTTP requests using Python. This will be used to send requests to the website we want to scrape.\n",
    "  \n",
    "- **BeautifulSoup**: A Python library for pulling data out of HTML and XML files. This will be used to parse the HTML content of the website and extract the data we need.\n",
    "\n",
    "- **Pandas**: A powerful data manipulation and analysis library that provides data structures like DataFrames, which are ideal for handling structured data. Think of this as Excel but for python where each column is a series and each row is a record. This will be used so that we can easily manipulate and clean our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Introduction to requests\n",
    "\n",
    "The requests library allows you to send HTTP requests using Python. It abstracts the complexities of making requests behind a simple API, making it easy to interact with web services and APIs.\n",
    "\n",
    "### 2.1 Making a basic GET request\n",
    "\n",
    "A get request is used to retrieve data from a server. Here is an example of how it would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://api.github.com\") \n",
    "# used to send a get request to the url specified\n",
    "print(response.status_code) # used to check if the request is successful\n",
    "# 200 is used to indicate that the request was successful\n",
    "# you may know 404 as the error code for a page not found\n",
    "print(response.json()) # used to print the content of the page\n",
    "#alternatively you can use response.text to print the content of the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requests.get` function sends a GET request to the specified URL. The response from the server is stored in the *response* variable.\n",
    "\n",
    "`Response Object`: The response object contains all the information returned by the server, including:\n",
    "\n",
    "`response.status_code`: The HTTP status code (e.g., 200 for success, 404 for not found).\n",
    "\n",
    "`response.text`: The content of the response as a string.\n",
    "\n",
    "`response.json()`: If the response content is in JSON format, this method parses it into a Python dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Handling Different HTTP Methods\n",
    "\n",
    "The requests library supports various HTTP methods, including POST, PUT, DELETE, etc.\n",
    "\n",
    "POST Request\n",
    "A POST request is used to send data to a server to create/update a resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://httpbin.org/post'\n",
    "data = {'key': 'value'}\n",
    "\n",
    "response = requests.post(url, data=data)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 PUT Request\n",
    "\n",
    "A PUT request is used to update a resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://httpbin.org/put'\n",
    "data = {'key': 'value'}\n",
    "\n",
    "response = requests.put(url, data=data)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 DELETE Request\n",
    "\n",
    "A DELETE request is used to delete a resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://httpbin.org/delete'\n",
    "\n",
    "response = requests.delete(url)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Using Headers with Requests\n",
    "\n",
    "When making HTTP requests, headers can be used to provide additional information to the server. \n",
    "\n",
    "Headers are key-value pairs that can include information such as the user agent, content type, and authorization tokens. Using headers can help you interact with web services and APIs more effectively.\n",
    "\n",
    "Here is an example of how to use headers with the `requests` library in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://teamacademy.nl/faq/'\n",
    "\n",
    "# Headers are used to send additional information to the server when processing a request\n",
    "headers = {\n",
    "    # This is used to identify the browser\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "    # We use this to get the page in html\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', \n",
    "    # We use this to get the page in english\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    # Used to get the page in the original form\n",
    "    'Accept-Encoding': 'identity'  \n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the `headers` dictionary contains several common headers:\n",
    "- `User-Agent`: Identifies the client software making the request.\n",
    "- `Accept`: Specifies the media types that are acceptable for the response.\n",
    "- `Accept-Language`: Specifies the preferred languages for the response.\n",
    "- `Accept-Encoding`: Specifies the content encoding that is acceptable for the response.\n",
    "\n",
    "By including these headers in the request, you can ensure that the server receives the necessary information to process the request correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Transition from Requests to BeautifulSoup\n",
    "\n",
    "While the `requests` library is excellent for making HTTP requests and retrieving the content of web pages, it does not provide tools for manipulating and extracting data from the HTML content. This is where `BeautifulSoup` comes in. `BeautifulSoup` is a Python library used for parsing HTML and XML documents and extracting data from them in a hierarchical and readable manner.\n",
    "\n",
    "### 3.1 Why Use BeautifulSoup?\n",
    "\n",
    "- **HTML Parsing**: BeautifulSoup can get the  HTML content and create a tree structure of html tags that can be used to extract data from HTML tags.\n",
    "- **Easy Navigation**: It provides methods to navigate and search that tree, making it easy to find and extract specific elements.\n",
    "- **Data Extraction**: You can extract data from HTML tags, attributes, and text content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # used to import the BeautifulSoup class from the bs4 module\n",
    "\n",
    "content = response.content # used to get the content of the page\n",
    "\n",
    "soup = BeautifulSoup(content, 'html.parser') # used to create a BeautifulSoup object\n",
    "\n",
    "print(soup.prettify()) # used to print the content of the page in a readable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Are we allowed to scrape any website?\n",
    "\n",
    "Web scraping can be a powerful tool for data collection and analysis, but it is important to understand the legal implications and ethical considerations associated with it.\n",
    "\n",
    "1. **Terms of Service**: Many websites have terms of service (ToS) that explicitly prohibit scraping. Violating these terms can lead to legal consequences, including being banned from the website or facing legal action.\n",
    "\n",
    "2. **Copyright Laws**: The content on websites is often protected by copyright laws. Scraping and using this content without permission can infringe on the copyright holder's rights.\n",
    "\n",
    "3. **Data Privacy**: Scraping personal data from websites can violate data privacy laws, such as the General Data Protection Regulation (GDPR) in the European Union. Ensure that you comply with relevant data protection regulations when scraping data.\n",
    "\n",
    "4. **Robots.txt**: Websites often use a `robots.txt` file to indicate which parts of the site can be crawled by web crawlers. While this file is not legally binding, it is considered good practice to respect the directives specified in `robots.txt`.\n",
    "\n",
    "All in all the basic rule of thumb is to keep scraping to a minimum to not overload the server.\n",
    "Here are some nerds arguing if its legal or not [\"reddit link\"](https://www.reddit.com/r/webscraping/comments/yrs5eq/can_you_just_scrape_any_website/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Get useful information from the HTML content\n",
    "\n",
    "Now that we have this output we would want to do something useful with it like extracting the data from the the printed content. BeautifulSoup comes in clutch here. It allows us to specify what we want to extract from the HTML content using various methods like `find()`, `find_all()`, and `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first <h2> tag\n",
    "first_h2 = soup.find('h2') \n",
    "# the h tag is used to define the header of a page \n",
    "# the number after the h indicates the size of the header\n",
    "if first_h2:\n",
    "    print(first_h2.text)\n",
    "else:\n",
    "    print('No <h2> tag found')\n",
    "\n",
    "print('----------------------')\n",
    "\n",
    "# find the next <h2> tag\n",
    "next_h2 = first_h2.find_next('h2')\n",
    "if next_h2:\n",
    "    print(next_h2.text)\n",
    "else:\n",
    "    print('No next <h2> tag found')\n",
    "\n",
    "print('----------------------')\n",
    "\n",
    "# Find all <p> tags\n",
    "all_p_tags = soup.find_all(['p']) # here we can also add more things to the list\n",
    "# the p tag is used to define a paragraph\n",
    "# using this we will find all the paragraphs in the page\n",
    "for p in all_p_tags:\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the select method to find all <p> tags with a specific class\n",
    "selected_paragraphs = soup.select('p.some-class')\n",
    "\n",
    "# Print the text content of each selected paragraph\n",
    "for paragraph in selected_paragraphs:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this can be found on the webpage that we are trying to scrape. Using inspect elements shows us all the HTML content of the webpage. We can use this to find the tags that we want to extract. or things that we want to focus on.\n",
    "Below is an example of how we can extract a specific element that we found using inspect elements.\n",
    "To find specific elements, we can use methods like `find()`, `find_all()`, and `select()`. Here are some examples:\n",
    "\n",
    "Common HTML elements include:\n",
    "- `<a>`: Defines a hyperlink\n",
    "- `<div>`: Defines a division or section\n",
    "- `<p>`: Defines a paragraph\n",
    "- `<h1>` to `<h6>`: Define HTML headings\n",
    "- `<span>`: Defines a section in a document\n",
    "- `<ul>`: Defines an unordered list\n",
    "- `<ol>`: Defines an ordered list\n",
    "- `<li>`: Defines a list item\n",
    "- `<table>`: Defines a table\n",
    "- `<tr>`: Defines a row in a table\n",
    "- `<td>`: Defines a cell in a table\n",
    "- `<th>`: Defines a header cell in a table\n",
    "- `<form>`: Defines an HTML form for user input\n",
    "- `<input>`: Defines an input control\n",
    "- `<button>`: Defines a clickable button\n",
    "- `<img>`: Embeds an image\n",
    "- `<nav>`: Defines navigation links\n",
    "- `<header>`: Defines a header for a document or section\n",
    "- `<footer>`: Defines a footer for a document or section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title\n",
    "title_div = soup.find('div', id='elementor-tab-title-26411')\n",
    "# the title_div is used to find the div with the id elementor-tab-title-26411\n",
    "# this div was found by inspecting the page\n",
    "if title_div:\n",
    "    #if we find it\n",
    "    titles = title_div.find_all('a', class_='elementor-accordion-title')\n",
    "    # we find all the a tags with the class elementor-accordion-title\n",
    "    title_texts = [title.text for title in titles] # gets all the text from the a tags\n",
    "    print(\"Titles:\", title_texts)\n",
    "else:\n",
    "    print(\"No title div found\")\n",
    "\n",
    "# Extract content and links\n",
    "content_div = soup.find('div', id='elementor-tab-content-26411')\n",
    "# the content_div is used to find the div with the id elementor-tab-content-26411\n",
    "if content_div:\n",
    "    # if we find it\n",
    "    paragraphs = content_div.get_text().strip() # gets all the text from the div\n",
    "    links = [(a.text, a['href']) for a in content_div.find_all('a', href=True)] # finds all the links in the div\n",
    "    print(\"\\nContent:\", paragraphs)\n",
    "    print(\"\\nLinks:\")\n",
    "    for text, url in links:\n",
    "        print(f\"- {text}: {url}\")\n",
    "else:\n",
    "    print(\"No content div found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now want to do something useful with the text that extracted from the webpage. Maybe save it to a file or interact with it in some productive way.\n",
    "\n",
    "This can be done by first formatting the data so that it is easier to work with. We can use the `pandas` library to do this.\n",
    "\n",
    "But first we need to decide what we would want to save. The data that can be saved here can be pairs of questions and answers. \n",
    "\n",
    "To do this we first need to select all the questions and answers from the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [] # used to store the questions\n",
    "answers = [] # used to store the answers\n",
    "\n",
    "for title in soup.find_all('a', class_='elementor-accordion-title'): \n",
    "    # finds all the a tags with the class elementor-accordion-title\n",
    "    # thats where the questions are stored\n",
    "    questions.append(title.text)\n",
    "\n",
    "for content in soup.find_all('div', class_='elementor-tab-content'):\n",
    "    # finds all the divs with the class elementor-tab-content\n",
    "    # thats where the answers are stored\n",
    "    answers.append(content.text)\n",
    "\n",
    "print(f\"First Question: {questions[0]}\")\n",
    "print(f\"First Answer: {answers[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we can finish this we need to know a little about `pandas` and how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 What is Pandas?\n",
    "\n",
    "Pandas is a powerful data manipulation and analysis library in Python. It provides data structures like DataFrames, which are ideal for handling structured data. Below is a demonstration of some basic operations you can perform using Pandas.\n",
    "Below is a demonstration of creating a DataFrame using Pandas.\n",
    "\n",
    "### 4.1 Creating a DataFrame \n",
    "\n",
    "A DataFrame is a 2d data structure that can store different types of data in columns. You can think of it as a spreadsheet in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = { # We can use a dictionary to create a DataFrame\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'], #This will add values to the first column\n",
    "    # The values in the list will be added to the DataFrame\n",
    "    # and the value on the left (Key) will be the name of the column\n",
    "    'Age': [25, 30, 35], #This will add values to the second column\n",
    "    'Gender': ['F', 'M', 'M'] #This will add values to the third column\n",
    "}\n",
    "df = pd.DataFrame(data) #This will create a DataFrame from the data we have\n",
    "df #This will display the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Save to file\n",
    "\n",
    "We can then save this to a file using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf='data/output.csv', index=False) \n",
    "# Here index is set to False as we dont need to save \n",
    "# the indexes of the data to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Reading Data from a File\n",
    "\n",
    "Pandas can read data from various file formats, such as CSV, Excel, and SQL databases. Here is an example of reading data from a CSV file using the one we created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('output.csv') #This will read the data from the file\n",
    "\n",
    "# we can check if the two dataframes are the same by using the equals method\n",
    "df.equals(other=new_df) #This will return True if the two DataFrames are the same\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Data Manipulation\n",
    "\n",
    "You can perform various data manipulation operations using Pandas. Here are examples of some common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows based on a condition\n",
    "filtered_df = df[df['Age'] > 25]\n",
    "print(\"Filtered DataFrame (Age > 25):\")\n",
    "print(filtered_df)\n",
    "\n",
    "# Adding a new column\n",
    "df['Country'] = ['USA', 'Canada', 'UK']\n",
    "print(\"\\nDataFrame with new column 'Country':\")\n",
    "print(df)\n",
    "\n",
    "# Dropping a column\n",
    "df_dropped = df.drop(columns=['Gender'])\n",
    "print(\"\\nDataFrame after dropping 'Gender' column:\")\n",
    "print(df_dropped)\n",
    "\n",
    "# Renaming columns\n",
    "df_renamed = df.rename(columns={'Name': 'Full Name', 'Age': 'Years'})\n",
    "print(\"\\nDataFrame with renamed columns:\")\n",
    "print(df_renamed)\n",
    "\n",
    "# Sorting the DataFrame by a column\n",
    "df_sorted = df.sort_values(by='Age', ascending=False)\n",
    "print(\"\\nDataFrame sorted by 'Age' in descending order:\")\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Data Cleaning\n",
    "\n",
    "Data cleaning is an essential step in data preprocessing. It involves identifying and correcting errors in the data to improve its quality and reliability. There are several common data cleaning operations that can be performed with dataframes. It all depends on the data you are working with. Here are some common data cleaning operations alongside when to use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Adding some dummy data with missing values\n",
    "df_cleaned.loc[3] = ['David', None, 'M', 'Australia']\n",
    "df_cleaned.loc[4] = [None, 28, None, 'India']\n",
    "# Loc is used to access a group of rows and columns by labels\n",
    "\n",
    "print(\"DataFrame with missing values:\")\n",
    "print(df_cleaned)\n",
    "\n",
    "# Dropping rows with missing values\n",
    "df_dropped_na = df_cleaned.dropna()\n",
    "print(\"\\nDataFrame after dropping rows with missing values:\")\n",
    "print(df_dropped_na)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this when missing data represents a small portion of your dataset (<5%) or when the missing values would significantly bias your analysis\n",
    "In the above example a better approach would be to fill the missing values with the replacing the values with either \"Unknown\" or the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the numeric columns with the mean value\n",
    "df_cleaned['Age'] = df_cleaned['Age'].fillna(df_cleaned['Age'].mean())\n",
    "print(\"\\nDataFrame after filling missing values in 'Age' column with mean value:\")\n",
    "print(df_cleaned)\n",
    "\n",
    "# Filling missing values with a default value\n",
    "df_filled_na = df_cleaned.fillna(value='Unknown')\n",
    "print(\"\\nDataFrame after filling missing values with 'Unknown':\")\n",
    "df_filled_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is better as it does not remove any data from the dataset. So whatever we do with this data next will be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Exercises\n",
    "\n",
    "#### 1. **Basic GET Request**:\n",
    " \n",
    "Write code to send a GET request to https://example.com and print the status code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to:\n",
    "# 1. Send GET request to https://example.com\n",
    "# 2. Print the status code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Find Second Header**: \n",
    "Write code using BeautifulSoup to find and print the text of the second h1 tag from this HTML string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "    <h1>Welcome</h1>\n",
    "    <h1>Second Header</h1>\n",
    "    <p>Some text</p>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Your code here to:\n",
    "# 1. Create BeautifulSoup object\n",
    "# 2. Find first h1 tag\n",
    "# 3. Find next h1 tag\n",
    "# 4. Print its text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Create simple DataFrame**\n",
    "\n",
    "Create a pandas DataFrame with 3 columns: 'Name', 'Age', and 'City' containing data for 2 people and save it to data/people.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to:\n",
    "# 1. Create dictionary with data\n",
    "# 2. Convert to DataFrame\n",
    "# 3. Display DataFrame\n",
    "# 4. Save DataFrame to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Create the QA dataframe**\n",
    "\n",
    "Based on the scraped data from before, create a pandas DataFrame with 2 columns: 'Question' and 'Answer' containing the questions and answers you extracted from the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to:\n",
    "# 1. Create DataFrame from lists\n",
    "# 2. display the second element of the DataFrame\n",
    "# 3. Save DataFrame to CSV file at location data/qa.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Data statistics**\n",
    "\n",
    "Return the total amount of questions and answers in the DataFrame, the average answer length, the longest answer and the shortest question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to:\n",
    "# 1. Count total Q&As\n",
    "# 2. Calculate average answer length\n",
    "# 3. Find longest answer and shortest question\n",
    "# 4. Display all statistics\n",
    "# hints, use len() to count the total Q&As\n",
    "# use apply() to calculate the average answer length\n",
    "# use idxmax() to find the longest answer\n",
    "# use idxmin() to find the shortest question\n",
    "# use print to display the statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
